---
title: "Joint Copula Bayesian Modelling"
author: "Andrea Ferrero"
format:
  pdf:
    number-sections: true
toc: true
toc-depth: 3
---

# Motivation

We are assuming the following joint distribution for a sequence of random variables $(X_1, \dots, X_n) \sim C_\theta(F(X_1), \dots, F(X_n))$, with C a parametric copula (potentially archimedean) and F the marginal of each $X_i$.

We know from simulation results that the likelihood peaks at the correct copula generating parameter if we take the realisations $U_1, \dots, U_n \sim C_\theta$. However, estimating the margin F from a realisation of $X_1, \dots, X_n$, to obtain the pseudo observations $\hat{U}_i = \hat{F}(X_i)$, leads to a likelihood which peaks at $\theta = 1$, the independence case with Gumbel copula.

```{r lik plot sim, echo = F, cache = TRUE, message=F}
set.seed(123)
library(copula)
# -----------------------------------------------------------
# 1. TRUE parameter and dimension
# -----------------------------------------------------------
n <- 100
theta_true <- 3

# -----------------------------------------------------------
# 2. Simulate ONE 100-dimensional vector from Gumbel copula
# -----------------------------------------------------------
cop <- gumbelCopula(param = theta_true, dim = n)
U <- as.numeric(rCopula(1, cop))   # true uniforms

# -----------------------------------------------------------
# 3. Generate X via lognormal margins
# -----------------------------------------------------------
mu <- 0
sigma <- 1
X <- qlnorm(U, meanlog = mu, sdlog = sigma)

# -----------------------------------------------------------
# 4A. Parametric margins
# -----------------------------------------------------------
mu_hat <- mean(log(X))
sigma_hat <- sd(log(X))
u_hat_param <- plnorm(X, meanlog = mu_hat, sdlog = sigma_hat)

# -----------------------------------------------------------
# 4B. ECDF pseudo-observations
# -----------------------------------------------------------
u_hat_ecdf <- rank(X) / (n + 1)

# -----------------------------------------------------------
# 5. Gumbel log-likelihood
# -----------------------------------------------------------
loglik_gumbel <- function(theta, u) {
  if (theta <= 1) return(-1e10)   # keep optimizer stable
  cop <- gumbelCopula(param = theta, dim = length(u)) 
  ll <- log(dCopula(u, copula = cop))
  return(ll)
}

# Negative log-likelihood for optimization
negLL <- function(theta, u) -loglik_gumbel(theta, u)

# -----------------------------------------------------------
# 6. Compute estimators via optim()
# -----------------------------------------------------------

# TRUE UNIFORMS
est_trueU <- optim(par = 5, fn = negLL, u = U, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# PARAMETRIC MARGINS
est_param <- optim(par = 5, fn = negLL, u = u_hat_param, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# ECDF PSEUDO-OBSERVATIONS
est_ecdf <- optim(par = 5, fn = negLL, u = u_hat_ecdf, method = "L-BFGS-B",
                   lower = 1.001, upper = 30)$par

# -----------------------------------------------------------
# 7. Likelihood curves for comparison
# -----------------------------------------------------------
theta_grid <- seq(1.01, 12, length.out = 200)

ll_trueU  <- sapply(theta_grid, loglik_gumbel, u = U)
ll_param  <- sapply(theta_grid, loglik_gumbel, u = u_hat_param)
ll_ecdf   <- sapply(theta_grid, loglik_gumbel, u = u_hat_ecdf)

```

```{r lik plot, echo = FALSE, fig.width=10, fig.height=8}
plot(theta_grid, ll_trueU, type="l", lwd=3, col="black",
     xlab=expression(theta), ylab="Log-likelihood",
     main = "Gumbel log-likelihood")

lines(theta_grid, ll_param, col="blue", lwd=2)
lines(theta_grid, ll_ecdf,  col="red",  lwd=2)

abline(v = theta_true, col="darkgreen", lty=2, lwd=2)

legend("topright",
       legend = c("True U", "Parametric Margin", "ECDF", "True theta"),
       col    = c("black", "blue", "red", "darkgreen"),
       lty    = c(1,1,1,2),
       lwd    = c(3,2,2,2))
       
```

The idea is to avoid the 2 steps procedure by performing estimation on the margin and the copula parameter simultaneously. A fully bayesian model would inherently consider both objects as targets, and might be able to consider the dependence structure while estimating the margin.

# Bayesian Model with Parametric Margin

The simplest approach consists in assuming a parametric form for the margin, that is $X_i \sim F_\varphi$, with $\varphi$ a generic parameter vector. Hierarchically, this yields the following structure:

$$
\begin{aligned}
\varphi \sim \pi_\varphi \, &, \quad \theta \sim \pi_\theta \\
(X_1, \dots, X_n) | \; \theta &, \varphi \sim C_\theta (F_\varphi (X_1), \dots, F_\varphi (X_n))
\end{aligned}
$$

## Posterior sampling with MCMC

We aim to sample from the joint log posterior:

$$
\log \pi(\theta,\varphi \mid x_{1:n})
\;\propto\;
\log \pi_\theta(\theta) +
\log \pi_\varphi(\varphi) +
\log c_\theta\!\left(F_\varphi(x_1), \dots, F_\varphi(x_n)\right) + 
\sum_{i=1}^n \log f_\varphi(x_i).
$$

We will call the full parameter vector:
$$
\boldsymbol{\phi} = (\theta,\varphi)\in\mathbb{R}^d.
$$

To sample from this posterior, a simple algorithm is a Random Walk Metropolisâ€“Hastings. Crucial is the proposal covariance $\Sigma_0$ set at the beginning. This determines the efficiency of the sampler while exploring the posterior distribution. Higher variance in the proposals implies the sampler will take with higher probability further steps, moving quicker around the posterior space but with a higher probability of ending up in regions of low posterior probability, increasing therefore the risk of rejection. In contrast, smaller proposal variance induces the sampler to take shorter steps, decreasing the risk of being rejected but at the same time exploring the posterior space more slowly. A good balance needs to be found and no simple solution exists.

For constrained parameters (like positive scale parameters, or the parameter of some copulas like the gumbel), transformations to unconstrained spaces are encouraged, with the caveat of adding the Jacobian adjustment to the posterior computation if the proposal distribution is in the original space.

**Algorithm 1: Metropolis--Hastings**

1. **Initialisation:**
   - Set the initial parameter vector $\boldsymbol{\phi}^{(0)}$
   - Compute the initial log-posterior $\ell_{\text{curr}} = \ell(\boldsymbol{\phi}^{(0)})$
   - Set the proposal covariance $\Sigma_0$

2. **For** $t = 1, \dots, N$:
   1. **Proposal:**  
      Draw  
      $$
      \boldsymbol{\phi}^\star \sim 
      \mathcal{N}_d\big(\boldsymbol{\phi}^{(t-1)}, \Sigma_0\big)
      $$

   2. **Evaluate proposed log-posterior:**  
      Compute $\ell^\star = \ell(\boldsymbol{\phi}^\star)$

   3. **Acceptance probability:**  
      $$
      \alpha = \min\{1, \exp(\ell^\star - \ell_{\text{curr}})\}
      $$

   4. **Accept/reject:**  
      Draw $u \sim \mathrm{Unif}(0,1)$  
      - If $u < \alpha$: set $\boldsymbol{\phi}^{(t)} = \boldsymbol{\phi}^\star$ and $\ell_{\text{curr}} = \ell^\star$ 
      - Else: set $\boldsymbol{\phi}^{(t)} = \boldsymbol{\phi}^{(t-1)}$

## Adaptive algorithms

To overcome the choice of the proposal covariance $\Sigma_0$, adaptive methods have been proposed. The idea behind adaptive MCMC is to learn the optimal proposal variance while the chain is running, or to target a specific acceptance rate considered to be optimal. In the case of multidimensional posterior, a standard acceptance rate to target is $\alpha^* = 0.234$.

There are two main schemes which often implemented: the Haario scheme and the Robbins-Monroe scheme.

The first one simply starts by an arbitrary covariance $\Sigma_0$ and updates it by computing the empirical covariance $\hat{\Sigma}$ of the chains, for example at every iteration. Instead, Robbins-Monroe scheme targets a specific acceptance rate $\alpha^*$ by updating the scale of the covariance iteratively, so the new covariance is of the form $\Sigma_t = \lambda_t \Sigma_0$.

Care must be taken when using adaptive methods as theoretical guarantees of ergodicity require some assumptions to be met, one of which is for example diminishing adaptation, to ensure that after a while adaptation practically stops and we ensure the Markov Chain will converge to its stationary distribution. Many suggest to simply use adaptation during the burn-in period and then freeze the obtained covariance to preserve ergodicity.

## Joint Copula Likelihood Intractability

The loglikelihood of our model is intractable due to the log density of the copula. After a certain sample size, this quantity cannot be easily computed, making inference impossible even for a moderately big sample.

```{r copula density error, error=TRUE}
library(copula)
set.seed(123)

n <- 1000
theta_true <- 2

cop <- gumbelCopula(param = theta_true, dim = n)
U <- as.numeric(rCopula(1, cop))   # true uniforms

dCopula(U, gumbelCopula(theta_true, dim = n))
```

## Simulation Based Inference - Bayesian Synthetic Likelihood

This branch of Bayesian inference overcomes the problem of intractable likelihood that makes posterior sampling otherwise impossible. Indeed, to compute $\alpha$ the likelihood needs to be evaluated for the proposed parameter $\phi^*$.

The main idea is to (efficiently) sample from the model multiple times and to summarise the obtained samples with summary statistics that are informative about the parameters of interest. With the Bayesian Synthetic Likelihood (BSL) approach, the joint distribution of the summary statistics is used to approximate the likelihood for the proposed parameter vector.

The original approach consisted in assuming the joint distribution of the statistics was multivariate normal. A refined, semiparametric approach estimates the marginal distributions with KDE and models eventual dependence with a Gaussian copula. See An et al. 2019.

This approach allows us to perform Metropolis-Hastings MCMC by approximating $\alpha$ with the approximate "synthetic" likelihood.

### Lognormal - Gumbel model

With the lognormal model, the parameters of interest are $\phi = (\mu, \sigma, \theta)$. We generated data with the true parameter vector $\phi_{true} = (0, 1, 2)$. The chosen statistics are the median $med(x)$, the median absolute deviation defined as $mad(x) = med(|x_i - med(x)|)$, which is a robust estimator for the variance, and the $max(x)$.

The MCMC results we have so far were obtained by running a burn-in period with the Haario adaptive scheme, and the obtained covariance was used to run a standard Metropolis-Hastings for the remaining iterations. The posterior distributions are roughly centered at the true generating values. MCMC efficiency could do with some improvement, as the Effective Sample Size is rather low compared to the number of MCMC iterations after burn in. Multiple chains were also run under a different sampling setup and mixing was found to be rather good. They are not reported in this case for simplicity, but it would be a necessary step if final conclusions were to be drawn from the Bayesian model.

```{r loading mcmc lognormal gumbel model, echo = FALSE, message = FALSE}
library(here)

sbi_folder <- here("sims/estim/joint/SBI")
sbi_res_folder <- here(sbi_folder, "res")
sbi_plot_folder <- here(sbi_folder, "plots")

load(here(sbi_res_folder, "semibsl_25kruns_10kburnin_with_haario_200sims_1kobs.Rdata"))

g_inv <- function(phi) {
  param <- c(
    mu    = phi[1],
    sigma = exp(phi[2]),
    theta = exp(phi[3]) + 1
  )
  # Ensure names are exactly what we want
  names(param) <- c("mu", "sigma", "theta")
  param
}
```

```{r res mcmc lognormal gumbel, echo = FALSE}
library(coda)
samples_natural <- t(apply(res$samples, 1, g_inv))

mcmc_samples <- mcmc(samples_natural)

mcmc_clean <- window(mcmc_samples, start = res$burn_in + 1, thin = 1)

summary(mcmc_clean)
```

```{r, echo = FALSE}
cat("Effective Sample Size \n (mu, sigma, theta): \n", effectiveSize(mcmc_clean))
```

```{r plot mcmc, echo = FALSE, fig.width=10, fig.height=8}
plot(mcmc_clean)
```

From here, the simplest conclusion we can already draw on the distribution of the maxima exploits the connection with the copula diagonal, namely:

$$
G(x) = P(M_n < x) = \delta_\theta(F_{\mu, \sigma}(x)) = F_{\mu, \sigma}(x)^{n^{\frac{1}{\theta}}}
$$

Direct use of posterior parameter draws allows us to construct the posterior distribution of the maximum:

![](C:\Users\Andrea Ferrero\extremesCopula\sims\estim\joint\SBI\plots\maxima_theta2_lognormal_25kiter_10kburnin_with_haario_200sims_1kobs.pdf){fig-align="center"}
